#!/usr/bin/env python3
"""
Enhanced BinaryInferno - åŸºäºè®ºæ–‡çš„å®Œæ•´å®ç°
ä¸»è¦æ”¹è¿›ï¼š
1. å®ç°å®Œæ•´çš„åŸå­æ£€æµ‹å™¨é›†æˆï¼ˆFloatã€Timestampã€Lengthï¼‰
2. æ”¹è¿›çš„ç†µåŸºå­—æ®µè¾¹ç•Œæ£€æµ‹å™¨
3. åŸºäºæ¨¡å¼çš„å˜é•¿å­—æ®µæ£€æµ‹å™¨
4. å›¾åŸºé›†æˆç®—æ³•
5. æ›´å‡†ç¡®çš„åè®®ç‰¹å®šä¼˜åŒ–
"""

import os
import sys
import numpy as np
import pandas as pd
import logging
import random
import math
import json
import time
from typing import Dict, List, Tuple, Optional, Set, Union
from collections import defaultdict, Counter
from pathlib import Path
import argparse
from sklearn.metrics import f1_score, accuracy_score
import warnings
from dataclasses import dataclass
from datetime import datetime, timezone

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')


@dataclass
class FieldDescription:
    """å­—æ®µæè¿°"""
    start: int
    end: int
    field_type: str
    confidence: float
    semantic_info: Optional[str] = None
    weight: float = 0.0


@dataclass
class Message:
    """æ¶ˆæ¯ç±»"""

    def __init__(self, data, source=None, destination=None, timestamp=None):
        self.data = data if isinstance(data, bytes) else bytes.fromhex(data.replace(' ', ''))
        self.source = source or "0.0.0.0:0"
        self.destination = destination or "0.0.0.0:0"
        self.timestamp = timestamp or 0
        self.id = random.randint(1000000, 9999999)


class AtomicDetectors:
    """åŸå­æ£€æµ‹å™¨é›†åˆ - åŸºäºè®ºæ–‡Section V"""

    def __init__(self, capture_time_range: Optional[Tuple[float, float]] = None):
        self.capture_time_range = capture_time_range

    def detect_floats(self, messages: List[Message]) -> List[FieldDescription]:
        """Floatæ£€æµ‹å™¨ - åŸºäºIEEE 754ç‰¹å¾"""
        logger.info("      è¿è¡ŒFloatæ£€æµ‹å™¨...")

        fields = []
        if len(messages) < 3:
            return fields

        # æ£€æŸ¥4å­—èŠ‚IEEE 754 float
        for offset in range(len(messages[0].data) - 3):
            if self._is_valid_float_slice(messages, offset, 4):
                fields.append(FieldDescription(
                    start=offset,
                    end=offset + 4,
                    field_type="float32",
                    confidence=0.8,
                    weight=len(messages) * 4
                ))

        return fields

    def _is_valid_float_slice(self, messages: List[Message], offset: int, width: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„floatåˆ‡ç‰‡"""
        try:
            values = []
            for msg in messages:
                if offset + width <= len(msg.data):
                    float_bytes = msg.data[offset:offset + width]
                    # è§£é‡Šä¸ºIEEE 754 float
                    import struct
                    try:
                        value = struct.unpack('>f', float_bytes)[0]
                        if not (math.isnan(value) or math.isinf(value)):
                            values.append(value)
                    except:
                        return False

            if len(values) < len(messages) * 0.8:
                return False

            # L-Ratioè®¡ç®— - è®ºæ–‡ä¸­çš„å…³é”®ç‰¹å¾
            return self._calculate_l_ratio(messages, offset, width)

        except:
            return False

    def _calculate_l_ratio(self, messages: List[Message], offset: int, width: int) -> bool:
        """è®¡ç®—L-Ratioç‰¹å¾"""
        try:
            exponent_freqs = [0] * 8
            significand_freqs = [0] * 23

            for msg in messages:
                if offset + width <= len(msg.data):
                    float_bytes = msg.data[offset:offset + width]
                    # åˆ†æä½æ¨¡å¼
                    bits = ''.join(format(b, '08b') for b in float_bytes)

                    # æŒ‡æ•°éƒ¨åˆ† (bits 1-8)
                    for i in range(1, 9):
                        if i < len(bits) and bits[i] == '1':
                            exponent_freqs[i - 1] += 1

                    # å°¾æ•°éƒ¨åˆ† (bits 9-31)
                    for i in range(9, 32):
                        if i < len(bits) and bits[i] == '1':
                            significand_freqs[i - 9] += 1

            max_exp_freq = max(exponent_freqs) if exponent_freqs else 1
            avg_sig_freq = sum(significand_freqs) / len(significand_freqs) if significand_freqs else 0

            l_ratio = avg_sig_freq / max_exp_freq if max_exp_freq > 0 else 0
            return 0.42 <= l_ratio <= 0.55  # è®ºæ–‡ä¸­çš„é˜ˆå€¼

        except:
            return False

    def detect_timestamps(self, messages: List[Message]) -> List[FieldDescription]:
        """Timestampæ£€æµ‹å™¨ - åŸºäºæ—¶é—´èŒƒå›´"""
        logger.info("      è¿è¡ŒTimestampæ£€æµ‹å™¨...")

        fields = []
        if not self.capture_time_range:
            return fields

        start_time, end_time = self.capture_time_range

        # æ£€æŸ¥4å­—èŠ‚Unix timestamp
        for offset in range(len(messages[0].data) - 3):
            if self._is_valid_timestamp_slice(messages, offset, 4, start_time, end_time):
                fields.append(FieldDescription(
                    start=offset,
                    end=offset + 4,
                    field_type="unix_timestamp",
                    confidence=0.9,
                    weight=len(messages) * 4
                ))

        return fields

    def _is_valid_timestamp_slice(self, messages: List[Message], offset: int, width: int,
                                  start_time: float, end_time: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„timestampåˆ‡ç‰‡"""
        try:
            import struct
            valid_count = 0

            for msg in messages:
                if offset + width <= len(msg.data):
                    ts_bytes = msg.data[offset:offset + width]

                    # å¤§ç«¯åºUnix timestamp
                    try:
                        timestamp = struct.unpack('>I', ts_bytes)[0]
                        if start_time <= timestamp <= end_time:
                            valid_count += 1
                    except:
                        pass

                    # å°ç«¯åºUnix timestamp
                    try:
                        timestamp = struct.unpack('<I', ts_bytes)[0]
                        if start_time <= timestamp <= end_time:
                            valid_count += 1
                    except:
                        pass

            return valid_count >= len(messages) * 0.8

        except:
            return False

    def detect_lengths(self, messages: List[Message]) -> List[FieldDescription]:
        """Lengthæ£€æµ‹å™¨ - ä¸¥æ ¼é•¿åº¦å­—æ®µ"""
        logger.info("      è¿è¡ŒLengthæ£€æµ‹å™¨...")

        fields = []

        # æ£€æŸ¥1å­—èŠ‚å’Œ2å­—èŠ‚é•¿åº¦å­—æ®µ
        for width in [1, 2]:
            for offset in range(len(messages[0].data) - width + 1):
                if self._is_valid_length_slice(messages, offset, width):
                    fields.append(FieldDescription(
                        start=offset,
                        end=offset + width,
                        field_type=f"length{width * 8}",
                        confidence=0.95,
                        weight=len(messages) * width
                    ))

        return fields

    def _is_valid_length_slice(self, messages: List[Message], offset: int, width: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„lengthåˆ‡ç‰‡"""
        try:
            import struct

            constants = set()
            for msg in messages:
                if offset + width <= len(msg.data):
                    if width == 1:
                        length_val = msg.data[offset]
                    elif width == 2:
                        # å°è¯•å¤§ç«¯åºå’Œå°ç«¯åº
                        length_val_be = struct.unpack('>H', msg.data[offset:offset + 2])[0]
                        length_val_le = struct.unpack('<H', msg.data[offset:offset + 2])[0]

                        # æ£€æŸ¥å“ªä¸ªæ›´åˆç†
                        if abs(length_val_be - len(msg.data)) < abs(length_val_le - len(msg.data)):
                            length_val = length_val_be
                        else:
                            length_val = length_val_le

                    # è®¡ç®—å¸¸æ•°k (length_val + k = message_length)
                    k = len(msg.data) - length_val
                    constants.add(k)

            # å¦‚æœæ‰€æœ‰æ¶ˆæ¯éƒ½æœ‰ç›¸åŒçš„kå€¼ï¼Œä¸”k >= 0ï¼Œåˆ™ä¸ºé•¿åº¦å­—æ®µ
            return len(constants) == 1 and list(constants)[0] >= 0

        except:
            return False


class FieldBoundaryDetector:
    """å­—æ®µè¾¹ç•Œæ£€æµ‹å™¨ - åŸºäºShannonç†µ"""

    def __init__(self, endianness: str = 'big'):
        self.endianness = endianness
        self.entropy_threshold = 1.0  # è®ºæ–‡ä¸­çš„é˜ˆå€¼

    def detect_boundaries(self, messages: List[Message]) -> List[int]:
        """åŸºäºç†µå·®å¼‚æ£€æµ‹å­—æ®µè¾¹ç•Œ"""
        logger.info("      è¿è¡Œç†µåŸºè¾¹ç•Œæ£€æµ‹å™¨...")

        if not messages:
            return [0]

        boundaries = [0]  # æ€»æ˜¯åŒ…å«èµ·å§‹ä½ç½®
        max_length = max(len(msg.data) for msg in messages)

        # è®¡ç®—æ¯ä¸ªä½ç½®çš„ç†µ
        entropies = []
        for pos in range(max_length):
            entropy = self._calculate_entropy_at_position(messages, pos)
            entropies.append(entropy)

        # æ£€æµ‹ç†µçš„æ˜¾è‘—å˜åŒ–
        for i in range(1, len(entropies) - 1):
            if self._is_entropy_boundary(entropies, i):
                boundaries.append(i)

        return sorted(list(set(boundaries)))

    def _calculate_entropy_at_position(self, messages: List[Message], position: int) -> float:
        """è®¡ç®—ç‰¹å®šä½ç½®çš„Shannonç†µ"""
        values = []
        for msg in messages:
            if position < len(msg.data):
                values.append(msg.data[position])

        if not values:
            return 0.0

        counts = Counter(values)
        total = len(values)
        entropy = 0.0

        for count in counts.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)

        return entropy

    def _is_entropy_boundary(self, entropies: List[float], position: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç†µè¾¹ç•Œ"""
        if position == 0 or position >= len(entropies) - 1:
            return False

        left_entropy = entropies[position - 1]
        right_entropy = entropies[position]

        if self.endianness == 'big':
            return left_entropy - right_entropy >= self.entropy_threshold
        else:
            return right_entropy - left_entropy >= self.entropy_threshold


class PatternDetector:
    """åŸºäºæ¨¡å¼çš„æ£€æµ‹å™¨ - ç”¨äºå˜é•¿å­—æ®µ"""

    def __init__(self):
        self.patterns = ['LV', 'TLV', 'LV*', 'TLV*']  # æ”¯æŒçš„æ¨¡å¼

    def detect_patterns(self, messages: List[Message]) -> List[FieldDescription]:
        """æ£€æµ‹åºåˆ—åŒ–æ¨¡å¼"""
        logger.info("      è¿è¡Œæ¨¡å¼æ£€æµ‹å™¨...")

        fields = []

        # ç®€åŒ–çš„æ¨¡å¼æ£€æµ‹ - æ£€æµ‹LVæ¨¡å¼
        for offset in range(len(messages[0].data) - 1):
            if self._detect_lv_pattern(messages, offset):
                # ä¼°è®¡LVå­—æ®µçš„é•¿åº¦
                pattern_length = self._estimate_lv_length(messages, offset)
                if pattern_length > 0:
                    fields.append(FieldDescription(
                        start=offset,
                        end=offset + pattern_length,
                        field_type="LV_pattern",
                        confidence=0.7,
                        weight=len(messages) * pattern_length
                    ))

        return fields

    def _detect_lv_pattern(self, messages: List[Message], offset: int) -> bool:
        """æ£€æµ‹Length-Valueæ¨¡å¼"""
        try:
            valid_count = 0

            for msg in messages:
                if offset + 1 < len(msg.data):
                    length_byte = msg.data[offset]

                    # æ£€æŸ¥é•¿åº¦å­—èŠ‚æ˜¯å¦åˆç†
                    if 0 < length_byte <= len(msg.data) - offset - 1:
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
                        if offset + 1 + length_byte <= len(msg.data):
                            valid_count += 1

            return valid_count >= len(messages) * 0.7

        except:
            return False

    def _estimate_lv_length(self, messages: List[Message], offset: int) -> int:
        """ä¼°è®¡LVæ¨¡å¼çš„æ€»é•¿åº¦"""
        try:
            lengths = []
            for msg in messages:
                if offset + 1 < len(msg.data):
                    length_byte = msg.data[offset]
                    total_length = 1 + length_byte  # é•¿åº¦å­—èŠ‚ + æ•°æ®
                    lengths.append(total_length)

            if lengths:
                return max(lengths)  # è¿”å›æœ€å¤§é•¿åº¦
            return 0

        except:
            return 0


class IntegrationAlgorithm:
    """é›†æˆç®—æ³• - åŸºäºå›¾çš„å†²çªè§£å†³"""

    def integrate_fields(self, all_fields: List[FieldDescription],
                         message_length: int) -> List[FieldDescription]:
        """é›†æˆæ‰€æœ‰æ£€æµ‹ç»“æœ"""
        logger.info("      è¿è¡Œå›¾åŸºé›†æˆç®—æ³•...")

        if not all_fields:
            return []

        # æ„å»ºæœ‰å‘æ— ç¯å›¾(DAG)
        graph = self._build_dag(all_fields, message_length)

        # è®¡ç®—æœ€å¤§æƒé‡è·¯å¾„
        optimal_path = self._find_maximum_path(graph)

        return optimal_path

    def _build_dag(self, fields: List[FieldDescription],
                   message_length: int) -> Dict:
        """æ„å»ºDAGç”¨äºå†²çªè§£å†³"""
        # æ·»åŠ æºèŠ‚ç‚¹å’Œæ±‡èšèŠ‚ç‚¹
        source = FieldDescription(
            start=-1, end=-1, field_type="SOURCE",
            confidence=1.0, weight=0.0
        )
        sink = FieldDescription(
            start=message_length, end=message_length,
            field_type="SINK", confidence=1.0, weight=0.0
        )

        all_nodes = [source] + fields + [sink]

        # æ„å»ºé‚»æ¥è¡¨
        graph = defaultdict(list)

        for i, node_a in enumerate(all_nodes):
            for j, node_b in enumerate(all_nodes):
                if i != j and self._strictly_precedes(node_a, node_b):
                    graph[i].append(j)

        return {
            'nodes': all_nodes,
            'edges': graph,
            'source': 0,
            'sink': len(all_nodes) - 1
        }

    def _strictly_precedes(self, field_a: FieldDescription,
                           field_b: FieldDescription) -> bool:
        """æ£€æŸ¥field_aæ˜¯å¦ä¸¥æ ¼åœ¨field_bä¹‹å‰"""
        return field_a.end <= field_b.start

    def _find_maximum_path(self, graph: Dict) -> List[FieldDescription]:
        """æ‰¾åˆ°æœ€å¤§æƒé‡è·¯å¾„"""
        nodes = graph['nodes']
        edges = graph['edges']
        source = graph['source']
        sink = graph['sink']

        # æ‹“æ‰‘æ’åº
        topo_order = self._topological_sort(edges, len(nodes))

        # åŠ¨æ€è§„åˆ’æ±‚æœ€å¤§è·¯å¾„
        dist = [-float('inf')] * len(nodes)
        parent = [-1] * len(nodes)
        dist[source] = 0

        for u in topo_order:
            if dist[u] != -float('inf'):
                for v in edges[u]:
                    weight = nodes[v].weight
                    if dist[u] + weight > dist[v]:
                        dist[v] = dist[u] + weight
                        parent[v] = u

        # é‡æ„è·¯å¾„
        path = []
        current = sink
        while parent[current] != -1:
            if nodes[current].field_type not in ['SOURCE', 'SINK']:
                path.append(nodes[current])
            current = parent[current]

        path.reverse()
        return path

    def _topological_sort(self, edges: Dict, num_nodes: int) -> List[int]:
        """æ‹“æ‰‘æ’åº"""
        in_degree = [0] * num_nodes
        for u in edges:
            for v in edges[u]:
                in_degree[v] += 1

        queue = [i for i in range(num_nodes) if in_degree[i] == 0]
        result = []

        while queue:
            u = queue.pop(0)
            result.append(u)

            for v in edges[u]:
                in_degree[v] -= 1
                if in_degree[v] == 0:
                    queue.append(v)

        return result


class EnhancedBinaryInfernoAlgorithm:
    """å¢å¼ºç‰ˆBinaryInfernoç®—æ³•"""

    def __init__(self):
        self.atomic_detectors = None
        self.boundary_detector = None
        self.pattern_detector = PatternDetector()
        self.integration_algorithm = IntegrationAlgorithm()

        # åè®®ç‰¹å¼‚æ€§å‚æ•°
        self.protocol_params = {
            'dns': {'min_field_size': 2, 'endianness': 'big'},
            'modbus': {'min_field_size': 1, 'endianness': 'big'},
            'smb': {'min_field_size': 1, 'endianness': 'little'},
            'smb2': {'min_field_size': 1, 'endianness': 'little'},
            'dhcp': {'min_field_size': 1, 'endianness': 'big'},
            'dnp3': {'min_field_size': 1, 'endianness': 'little'},
            's7comm': {'min_field_size': 1, 'endianness': 'big'},
            'ftp': {'min_field_size': 1, 'endianness': 'big'},
            'tls': {'min_field_size': 1, 'endianness': 'big'}
        }

    def extract_fields(self, messages: List[Message], protocol_name: str = None,
                       capture_time: Optional[Tuple] = None,
                       endianness: str = 'big') -> List[List[int]]:
        """ä¸»è¦çš„å­—æ®µæå–æ–¹æ³•"""
        logger.info(f"ğŸ” å¢å¼ºç‰ˆBinaryInfernoåˆ†æ {len(messages)} ä¸ªæ¶ˆæ¯...")

        if not messages:
            return []

        # åº”ç”¨åè®®ç‰¹å¼‚æ€§å‚æ•°
        if protocol_name and protocol_name in self.protocol_params:
            params = self.protocol_params[protocol_name]
            endianness = params.get('endianness', endianness)

        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.atomic_detectors = AtomicDetectors(capture_time)
        self.boundary_detector = FieldBoundaryDetector(endianness)

        results = []

        for msg in messages:
            # 1. è¿è¡ŒåŸå­æ£€æµ‹å™¨
            atomic_fields = []
            atomic_fields.extend(self.atomic_detectors.detect_floats([msg]))
            atomic_fields.extend(self.atomic_detectors.detect_timestamps([msg]))
            atomic_fields.extend(self.atomic_detectors.detect_lengths([msg]))

            # 2. è¿è¡Œè¾¹ç•Œæ£€æµ‹å™¨
            boundaries = self.boundary_detector.detect_boundaries([msg])
            boundary_fields = []
            for i in range(len(boundaries) - 1):
                boundary_fields.append(FieldDescription(
                    start=boundaries[i],
                    end=boundaries[i + 1],
                    field_type="boundary_field",
                    confidence=0.5,
                    weight=boundaries[i + 1] - boundaries[i]
                ))

            # 3. è¿è¡Œæ¨¡å¼æ£€æµ‹å™¨
            pattern_fields = self.pattern_detector.detect_patterns([msg])

            # 4. é›†æˆæ‰€æœ‰ç»“æœ
            all_fields = atomic_fields + boundary_fields + pattern_fields
            integrated_fields = self.integration_algorithm.integrate_fields(
                all_fields, len(msg.data)
            )

            # 5. è½¬æ¢ä¸ºè¾¹ç•Œåˆ—è¡¨
            boundaries = [0]
            for field in integrated_fields:
                if field.start > 0:
                    boundaries.append(field.start)
                if field.end < len(msg.data):
                    boundaries.append(field.end)

            boundaries.append(len(msg.data))
            boundaries = sorted(list(set(boundaries)))

            # 6. åå¤„ç†å’ŒéªŒè¯
            final_boundaries = self._postprocess_boundaries(
                boundaries, msg, protocol_name
            )

            results.append(final_boundaries)

        return results

    def _postprocess_boundaries(self, boundaries: List[int],
                                message: Message, protocol_name: str) -> List[int]:
        """åå¤„ç†è¾¹ç•Œ"""
        if len(boundaries) <= 2:
            return boundaries

        # ç§»é™¤è¿‡å°çš„å­—æ®µ
        min_field_size = 1
        if protocol_name in self.protocol_params:
            min_field_size = self.protocol_params[protocol_name]['min_field_size']

        filtered = [boundaries[0]]
        for i in range(1, len(boundaries)):
            if boundaries[i] - filtered[-1] >= min_field_size:
                filtered.append(boundaries[i])

        # åº”ç”¨åè®®ç‰¹å®šè§„åˆ™
        if protocol_name == 'dns' and len(message.data) >= 12:
            # DNSå›ºå®š12å­—èŠ‚å¤´éƒ¨
            if 12 not in filtered:
                filtered.append(12)
                filtered.sort()
        elif protocol_name == 'modbus' and len(message.data) >= 7:
            # Modbus MBAPå¤´éƒ¨7å­—èŠ‚
            if 7 not in filtered:
                filtered.append(7)
                filtered.sort()
        elif protocol_name in ['smb', 'smb2'] and len(message.data) >= 8:
            # SMBå¤´éƒ¨å­—æ®µ
            for pos in [4, 8]:
                if pos not in filtered and pos < len(message.data):
                    filtered.append(pos)
            filtered.sort()

        return filtered


class EnhancedBinaryInfernoDataLoader:
    """å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨"""

    def __init__(self, data_root: str = "../../Msg2"):
        self.data_root = Path(data_root)
        self.supported_protocols = [
            'smb', 'smb2', 'dns', 's7comm', 'dnp3',
            'modbus', 'ftp', 'tls', 'dhcp'
        ]

    def load_protocol_data(self, protocol_name: str) -> List[Dict]:
        """åŠ è½½åè®®æ•°æ®"""
        logger.info(f"ğŸ“Š åŠ è½½ {protocol_name.upper()} åè®®æ•°æ®...")

        csv_path = self.data_root / "csv" / protocol_name.lower()

        if not csv_path.exists():
            logger.warning(f"   âŒ CSVç›®å½•ä¸å­˜åœ¨: {csv_path}")
            return []

        data = []
        csv_files = list(csv_path.glob("*.csv"))

        if not csv_files:
            logger.warning(f"   âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶: {csv_path}")
            return []

        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"   ğŸ“ è¯»å–æ–‡ä»¶: {csv_file.name} ({len(df)} è¡Œ)")

                for _, row in df.iterrows():
                    try:
                        boundaries_str = str(row['Boundaries'])
                        if boundaries_str and boundaries_str != 'nan':
                            boundaries = [int(b.strip()) for b in boundaries_str.split(',')]
                        else:
                            boundaries = [0]

                        hex_data = str(row['HexData'])
                        data_length = len(bytes.fromhex(hex_data))

                        if 0 not in boundaries:
                            boundaries.insert(0, 0)
                        if data_length not in boundaries:
                            boundaries.append(data_length)

                        boundaries = sorted(list(set(boundaries)))

                        sample = {
                            'raw_data': hex_data,
                            'protocol': protocol_name.lower(),
                            'bytes': bytes.fromhex(hex_data),
                            'length': data_length,
                            'message_type': str(row.get('FunctionCode', 'unknown')),
                            'ground_truth_boundaries': boundaries,
                            'semantic_types': self._parse_semantic_info(row.get('SemanticTypes', '{}')),
                            'semantic_functions': self._parse_semantic_info(row.get('SemanticFunctions', '{}'))
                        }
                        data.append(sample)

                    except Exception as e:
                        logger.warning(f"   âš ï¸ è§£æè¡Œæ•°æ®é”™è¯¯: {e}")
                        continue

            except Exception as e:
                logger.error(f"   âŒ è¯»å–CSVæ–‡ä»¶é”™è¯¯: {e}")
                continue

        logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®")
        return data

    def _parse_semantic_info(self, semantic_str: str) -> Dict:
        """è§£æè¯­ä¹‰ä¿¡æ¯"""
        try:
            if semantic_str and semantic_str != 'nan':
                return json.loads(semantic_str)
            return {}
        except:
            return {}


class EnhancedBinaryInfernoEvaluator:
    """å¢å¼ºç‰ˆè¯„ä¼°å™¨"""

    def __init__(self):
        self.boundary_tolerance = 1

    def evaluate_boundaries(self, predicted_boundaries: List[int],
                            ground_truth_boundaries: List[int],
                            sequence_length: int) -> Dict[str, float]:
        """è¯„ä¼°è¾¹ç•Œæ£€æµ‹æ€§èƒ½"""

        # æ ‡å‡†è¯„ä¼°
        pred_positions = set(predicted_boundaries)
        true_positions = set(ground_truth_boundaries)

        # å‡†ç¡®ç‡è®¡ç®—
        correct_positions = 0
        for pos in range(sequence_length):
            pred_is_boundary = pos in pred_positions
            true_is_boundary = pos in true_positions
            if pred_is_boundary == true_is_boundary:
                correct_positions += 1

        accuracy = correct_positions / sequence_length if sequence_length > 0 else 0

        # ç²¾ç¡®ç‡è®¡ç®—
        if len(predicted_boundaries) > 0:
            true_positives = len(true_positions & pred_positions)
            precision = true_positives / len(predicted_boundaries)
        else:
            precision = 0

        # å¬å›ç‡è®¡ç®—
        if len(ground_truth_boundaries) > 0:
            true_positives = len(true_positions & pred_positions)
            recall = true_positives / len(ground_truth_boundaries)
        else:
            recall = 0

        # F1åˆ†æ•°
        if precision + recall > 0:
            f1_score = 2 * precision * recall / (precision + recall)
        else:
            f1_score = 0

        # å®Œç¾ç‡ - å­—æ®µçº§åˆ«çš„ç²¾ç¡®åŒ¹é…
        pred_fields = self._boundaries_to_fields(predicted_boundaries, sequence_length)
        true_fields = self._boundaries_to_fields(ground_truth_boundaries, sequence_length)

        if len(true_fields) > 0:
            perfect_matches = len(set(pred_fields) & set(true_fields))
            perfection = perfect_matches / len(true_fields)
        else:
            perfection = 0

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'perfection': perfection
        }

    def _boundaries_to_fields(self, boundaries: List[int], length: int) -> List[Tuple[int, int]]:
        """å°†è¾¹ç•Œè½¬æ¢ä¸ºå­—æ®µ"""
        if not boundaries:
            return [(0, length)] if length > 0 else []

        fields = []
        boundaries = sorted(set(boundaries))

        for i in range(len(boundaries)):
            start = boundaries[i]
            if i < len(boundaries) - 1:
                end = boundaries[i + 1]
            else:
                end = length

            if start < end and start < length:
                fields.append((start, min(end, length)))

        return fields


class EnhancedBinaryInfernoExperiment:
    """å¢å¼ºç‰ˆå®éªŒç®¡ç†å™¨"""

    def __init__(self, data_root: str = "../../Msg2"):
        self.data_loader = EnhancedBinaryInfernoDataLoader(data_root)
        self.algorithm = EnhancedBinaryInfernoAlgorithm()
        self.evaluator = EnhancedBinaryInfernoEvaluator()

        self.protocols = [
            'smb', 'smb2', 'dns', 's7comm', 'dnp3',
            'modbus', 'ftp', 'tls', 'dhcp'
        ]

        self.results = {}

    def run_experiments(self, protocols: List[str] = None, max_samples: int = None):
        """è¿è¡Œå®éªŒ"""
        if protocols is None:
            protocols = self.protocols

        logger.info("ğŸš€ å¢å¼ºç‰ˆBinaryInfernoå®éªŒå¼€å§‹")
        logger.info("=" * 70)

        for protocol in protocols:
            logger.info(f"\nğŸ“Š æµ‹è¯•åè®®: {protocol.upper()}")
            logger.info("-" * 50)

            data = self.data_loader.load_protocol_data(protocol)

            if not data:
                logger.warning(f"   âŒ è·³è¿‡ {protocol}: æ— æ•°æ®")
                continue

            if max_samples and len(data) > max_samples:
                data = random.sample(data, max_samples)
                logger.info(f"   ğŸ“ é™åˆ¶æ ·æœ¬æ•°é‡: {max_samples}")

            messages = []
            for sample in data:
                msg = Message(sample['raw_data'])
                messages.append(msg)

            try:
                logger.info(f"   ğŸ” è¿è¡Œå¢å¼ºç‰ˆBinaryInfernoç®—æ³•...")

                # è®¾ç½®æ•è·æ—¶é—´èŒƒå›´ (ä½¿ç”¨å½“å‰æ—¶é—´å‰åä¸€å¤©)
                current_time = int(time.time())
                capture_time = (current_time - 86400, current_time + 86400)

                predicted_boundaries = self.algorithm.extract_fields(
                    messages, protocol, capture_time, 'big'
                )

                logger.info(f"   ğŸ“ˆ è¯„ä¼°æ€§èƒ½...")
                all_metrics = []

                for sample, pred_boundaries in zip(data, predicted_boundaries):
                    true_boundaries = sample['ground_truth_boundaries']
                    length = sample['length']

                    metrics = self.evaluator.evaluate_boundaries(
                        pred_boundaries, true_boundaries, length
                    )
                    all_metrics.append(metrics)

                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                avg_metrics = {}
                for key in ['accuracy', 'precision', 'recall', 'f1_score', 'perfection']:
                    values = [m[key] for m in all_metrics if not np.isnan(m[key])]
                    avg_metrics[key] = np.mean(values) if values else 0.0

                self.results[protocol] = {
                    'sample_count': len(data),
                    'metrics': avg_metrics
                }

                logger.info(f"   âœ… ç»“æœ:")
                logger.info(f"      æ ·æœ¬æ•°é‡: {len(data)}")
                logger.info(f"      å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
                logger.info(f"      ç²¾ç¡®ç‡: {avg_metrics['precision']:.4f}")
                logger.info(f"      å¬å›ç‡: {avg_metrics['recall']:.4f}")
                logger.info(f"      F1åˆ†æ•°: {avg_metrics['f1_score']:.4f}")
                logger.info(f"      å®Œç¾ç‡: {avg_metrics['perfection']:.4f}")

            except Exception as e:
                logger.error(f"   âŒ å¤„ç† {protocol} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                self.results[protocol] = {
                    'sample_count': len(data),
                    'metrics': {'accuracy': 0, 'precision': 0, 'recall': 0,
                                'f1_score': 0, 'perfection': 0},
                    'error': str(e)
                }

    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        logger.info(f"\n" + "=" * 70)
        logger.info("ğŸ“Š å¢å¼ºç‰ˆBinaryInfernoå®éªŒæŠ¥å‘Š")
        logger.info("=" * 70)

        if not self.results:
            logger.warning("âŒ æ²¡æœ‰å®éªŒç»“æœ")
            return

        report_data = []
        for protocol, result in self.results.items():
            metrics = result['metrics']
            report_data.append({
                'Protocol': protocol.upper(),
                'Samples': result['sample_count'],
                'Accuracy': f"{metrics['accuracy']:.4f}",
                'Precision': f"{metrics['precision']:.4f}",
                'Recall': f"{metrics['recall']:.4f}",
                'F1-score': f"{metrics['f1_score']:.4f}",
                'Perfection': f"{metrics['perfection']:.4f}"
            })

        df = pd.DataFrame(report_data)
        print("\nå¢å¼ºç‰ˆBinaryInfernoå®éªŒç»“æœè¡¨æ ¼:")
        print(df.to_string(index=False))

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        avg_metrics = {}
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'perfection']:
            values = [r['metrics'][metric] for r in self.results.values() if metric in r['metrics']]
            avg_metrics[metric] = np.mean(values) if values else 0.0

        logger.info(f"\nğŸ¯ æ€»ä½“æ€§èƒ½:")
        logger.info(f"   å¹³å‡å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
        logger.info(f"   å¹³å‡ç²¾ç¡®ç‡: {avg_metrics['precision']:.4f}")
        logger.info(f"   å¹³å‡å¬å›ç‡: {avg_metrics['recall']:.4f}")
        logger.info(f"   å¹³å‡F1åˆ†æ•°: {avg_metrics['f1_score']:.4f}")
        logger.info(f"   å¹³å‡å®Œç¾ç‡: {avg_metrics['perfection']:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆBinaryInfernoå®éªŒ')

    parser.add_argument('--protocols', nargs='+',
                        choices=['smb', 'smb2', 'dns', 's7comm', 'dnp3',
                                 'modbus', 'ftp', 'tls', 'dhcp'],
                        help='è¦æµ‹è¯•çš„åè®®åˆ—è¡¨')

    parser.add_argument('--max-samples', type=int, default=None,
                        help='æ¯ä¸ªåè®®çš„æœ€å¤§æ ·æœ¬æ•°')

    parser.add_argument('--data-root', type=str, default="../../Msg2",
                        help='æ•°æ®æ ¹ç›®å½•è·¯å¾„')

    args = parser.parse_args()

    experiment = EnhancedBinaryInfernoExperiment(args.data_root)

    logger.info(f"ğŸŒŸ å¢å¼ºç‰ˆBinaryInfernoå®éªŒè®¾ç½®:")
    logger.info(f"   æ•°æ®ç›®å½•: {args.data_root}")
    logger.info(f"   æµ‹è¯•åè®®: {args.protocols or 'ALL'}")
    logger.info(f"   æœ€å¤§æ ·æœ¬: {args.max_samples or 'ALL'}")

    experiment.run_experiments(protocols=args.protocols, max_samples=args.max_samples)
    experiment.generate_report()

    logger.info("\nâœ… å¢å¼ºç‰ˆBinaryInfernoå®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()