"""
Experiment 2: Multi-Protocol Enhanced Version

Updates:
1. âœ… æ·»åŠ  BGP å’Œ ZigBee åè®®
2. âœ… ä¼˜åŒ– Ground Truthï¼ˆæ¸è¿›å¼GTç­–ç•¥ï¼‰
3. âœ… æ·»åŠ è°ƒè¯•æ¨¡å¼è¯Šæ–­ä½Perfectioné—®é¢˜
"""

import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import logging
import numpy as np
import pandas as pd
from typing import List, Dict
from ground_truth_definitions import *
from neupre_core import simulate_neupre_segmentation, compute_metrics
from neupre import setup_logging
from utils.pcap_loader import PCAPDataLoader

# ==================== åè®®é…ç½® ====================
PROTOCOL_CONFIGS = {
    # ICS åè®®
    'modbus': {'pcap': 'in-modbus-pcaps/libmodbus-bandwidth_server-rand_client.pcap', 
               'gt_func': 'get_modbus_gt', 'type': 'ICS'},
    'dnp3': {'pcap': 'in-dnp3-pcaps/BinInf_dnp3_1000.pcap', 
             'gt_func': 'get_dnp3_gt', 'type': 'ICS'},
    's7comm': {'pcap': 'in-s7comm-pcaps/s7comm.pcap', 
               'gt_func': 'get_s7comm_gt', 'type': 'ICS'},
    'iec104': {'pcap': 'in-iec104-pcaps/iec104.pcap', 
               'gt_func': 'get_iec104_gt', 'type': 'ICS'},
    'lon': {'pcap': 'in-lon-pcaps/lon.pcap', 
            'gt_func': 'get_lon_gt', 'type': 'ICS'},
    
    # ç½‘ç»œåè®®
    # 'dhcp': {'pcap': 'in-dhcp-pcaps/BinInf_dhcp_1000.pcap', 
    #          'gt_func': 'get_dhcp_gt', 'type': 'Network'},
    'dns': {'pcap': 'in-dns-pcaps/SMIA_DNS_1000.pcap', 
            'gt_func': 'get_dns_gt', 'type': 'Network'},
    'rtp': {'pcap': 'in-rtp-pcaps/RTP_1000.pcap', 
            'gt_func': 'get_rtp_gt', 'type': 'Network'},
    'tcp': {'pcap': 'in-tcp-pcaps/SMIA_TCP_part_1000.pcap', 
            'gt_func': 'get_tcp_gt', 'type': 'Network'},
    'bgp': {'pcap': 'in-bgp-pcaps/bgp.pcap',  # â­ æ–°å¢
            'gt_func': 'get_bgp_gt', 'type': 'Network'},
    
    # æ–‡ä»¶/åº”ç”¨åè®®
    # 'smb2': {'pcap': 'in-smb2-pcaps/samba.pcap', 
    #          'gt_func': 'get_smb2_gt', 'type': 'File'},
    # 'smb': {'pcap': 'in-smb-pcaps/BinInf_smb_1000.pcap', 
    #         'gt_func': 'get_smb_gt', 'type': 'File'},
    
    # # IoT åè®®
    # 'zigbee': {'pcap': 'in-zigbee-pcaps/zigbeelxw.pcap',  # â­ æ–°å¢
    #            'gt_func': 'get_zigbee_gt', 'type': 'IoT'}
}


def find_pcap(pattern: str, data_dir: str) -> str:
    """æŸ¥æ‰¾ PCAP æ–‡ä»¶"""
    import glob
    if '*' in pattern:
        matches = glob.glob(os.path.join(data_dir, pattern))
        return os.path.relpath(matches[0], data_dir) if matches else None
    return pattern


def process_protocol(name: str, config: Dict, loader: PCAPDataLoader, 
                     max_msg: int = 1000, debug: bool = False):
    """å¤„ç†å•ä¸ªåè®®"""
    logging.info(f"\n{'='*80}\nProcessing {name.upper()}\n{'='*80}")
    
    try:
        pcap_path = find_pcap(config['pcap'], str(loader.data_dir))
        if not pcap_path:
            logging.warning(f"PCAP not found for {name}")
            return None
        
        messages = loader.load_messages(pcap_path, max_messages=max_msg)
        if not messages or len(messages) < 10:
            logging.warning(f"Insufficient data: {len(messages) if messages else 0} messages")
            return None
        
        logging.info(f"Loaded {len(messages)} messages")
        
        # Ground Truth
        gt_func = globals()[config['gt_func']]
        ground_truth = [gt_func(m) for m in messages]
        
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥GTåˆ†å¸ƒ
        if debug:
            avg_gt_fields = np.mean([len(gt) - 1 for gt in ground_truth])
            logging.info(f"ğŸ“Š GT Stats: Avg {avg_gt_fields:.1f} fields/message")
            logging.info(f"   Sample GT: {ground_truth[0]}")
        
        # NeuPRE åˆ†å‰²
        predictions = simulate_neupre_segmentation(messages)
        
        # ğŸ” è°ƒè¯•ï¼šå¯¹æ¯”GT vs Pred
        if debug:
            avg_pred_fields = np.mean([len(p) - 1 for p in predictions])
            logging.info(f"ğŸ“Š Pred Stats: Avg {avg_pred_fields:.1f} fields/message")
            logging.info(f"   Sample Pred: {predictions[0]}")
            
            # è¯¦ç»†å¯¹æ¯”
            debug_gt_coverage(name, predictions, ground_truth)
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = compute_metrics(predictions, ground_truth)
        
        logging.info(f"Results: Acc={metrics['accuracy']:.3f} F1={metrics['f1']:.3f} "
                    f"Correct={metrics['correctness']:.3f} Perf={metrics['perfect_match']:.3f}")
        
        return {
            'protocol': name,
            'type': config['type'],
            'num_messages': len(messages),
            'metrics': metrics,
            'predictions': predictions if debug else None,
            'ground_truth': ground_truth if debug else None
        }
    
    except Exception as e:
        logging.error(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_table(results: Dict, output_dir: str):
    """ç”Ÿæˆç»“æœè¡¨æ ¼"""
    rows = []
    for name in sorted(results.keys()):
        r = results[name]
        m = r['metrics']
        rows.append({
            'Protocol': name.upper(),
            'Type': r['type'],
            'Msgs': r['num_messages'],
            'Acc': f"{m['accuracy']:.3f}",
            'P': f"{m['precision']:.3f}",
            'R': f"{m['recall']:.3f}",
            'F1': f"{m['f1']:.3f}",
            'Correct': f"{m['correctness']:.3f}",
            'Perf': f"{m['perfect_match']:.3f}"
        })
    
    # åˆ†ç±»å¹³å‡
    for ptype in sorted(set(r['type'] for r in results.values())):
        subset = [r for r in results.values() if r['type'] == ptype]
        rows.append({
            'Protocol': f'--- {ptype} Avg ---',
            'Type': ptype,
            'Msgs': sum(r['num_messages'] for r in subset),
            'Acc': f"{np.mean([r['metrics']['accuracy'] for r in subset]):.3f}",
            'P': f"{np.mean([r['metrics']['precision'] for r in subset]):.3f}",
            'R': f"{np.mean([r['metrics']['recall'] for r in subset]):.3f}",
            'F1': f"{np.mean([r['metrics']['f1'] for r in subset]):.3f}",
            'Correct': f"{np.mean([r['metrics']['correctness'] for r in subset]):.3f}",
            'Perf': f"{np.mean([r['metrics']['perfect_match'] for r in subset]):.3f}"
        })
    
    # æ€»å¹³å‡
    rows.append({
        'Protocol': '=== OVERALL ===',
        'Type': 'All',
        'Msgs': sum(r['num_messages'] for r in results.values()),
        'Acc': f"{np.mean([r['metrics']['accuracy'] for r in results.values()]):.3f}",
        'P': f"{np.mean([r['metrics']['precision'] for r in results.values()]):.3f}",
        'R': f"{np.mean([r['metrics']['recall'] for r in results.values()]):.3f}",
        'F1': f"{np.mean([r['metrics']['f1'] for r in results.values()]):.3f}",
        'Correct': f"{np.mean([r['metrics']['correctness'] for r in results.values()]):.3f}",
        'Perf': f"{np.mean([r['metrics']['perfect_match'] for r in results.values()]):.3f}"
    })
    
    df = pd.DataFrame(rows)
    
    # ä¿å­˜
    csv_path = os.path.join(output_dir, 'results_enhanced.csv')
    df.to_csv(csv_path, index=False)
    
    print("\n" + "="*100)
    print("RESULTS (Optimized GT + DYNPRE Perfection)")
    print("="*100)
    print(df.to_string(index=False))
    print("="*100)
    
    logging.info(f"Saved to: {csv_path}")
    
    # ğŸ¯ æ€§èƒ½åˆ†æ
    print("\nğŸ“Š Performance Analysis:")
    avg_perf = np.mean([r['metrics']['perfect_match'] for r in results.values()])
    high_perf = [n for n, r in results.items() if r['metrics']['perfect_match'] > 0.3]
    low_perf = [n for n, r in results.items() if r['metrics']['perfect_match'] < 0.1]
    
    print(f"  Average Perfection: {avg_perf:.3f}")
    print(f"  High performers (>30%): {high_perf}")
    print(f"  Low performers (<10%): {low_perf}")


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--filter', default=None, help='ics/network/file/iot or protocol name')
    parser.add_argument('--max-messages', type=int, default=1000)
    parser.add_argument('--output-dir', default='./experiments/exp2_enhanced')
    parser.add_argument('--all', action='store_true', help='Test all protocols')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    args = parser.parse_args()
    
    setup_logging(level=logging.INFO)
    
    logging.info("="*80)
    logging.info("Experiment 2: Enhanced Multi-Protocol (Optimized GT)")
    if args.debug:
        logging.info("ğŸ” DEBUG MODE ENABLED")
    logging.info("="*80)
    
    os.makedirs(args.output_dir, exist_ok=True)
    loader = PCAPDataLoader(data_dir='../data')
    
    # é€‰æ‹©åè®®
    if args.all or args.filter is None:
        selected = PROTOCOL_CONFIGS
    elif args.filter.lower() in ['ics', 'network', 'file', 'iot']:
        selected = {k: v for k, v in PROTOCOL_CONFIGS.items() 
                   if v['type'].lower() == args.filter.lower()}
    elif args.filter in PROTOCOL_CONFIGS:
        selected = {args.filter: PROTOCOL_CONFIGS[args.filter]}
    else:
        print(f"âŒ Unknown filter: {args.filter}")
        print(f"Available: {list(PROTOCOL_CONFIGS.keys())}")
        return
    
    logging.info(f"Testing {len(selected)} protocols: {list(selected.keys())}")
    
    # å¤„ç†
    results = {}
    for name, config in selected.items():
        result = process_protocol(name, config, loader, args.max_messages, args.debug)
        if result:
            results[name] = result
    
    # è¾“å‡º
    if results:
        generate_table(results, args.output_dir)
        print(f"\nâœ… Processed {len(results)}/{len(selected)} protocols")
        
        # ğŸ” å¦‚æœæœ‰ä½æ€§èƒ½åè®®ï¼Œå»ºè®®ä½¿ç”¨debugæ¨¡å¼
        low_perf = [n for n, r in results.items() if r['metrics']['perfect_match'] < 0.1]
        if low_perf and not args.debug:
            print(f"\nğŸ’¡ Tip: Low performance on {low_perf}")
            print(f"   Run with --debug to see detailed GT vs Pred comparison")
    else:
        print("âŒ No results generated")


if __name__ == '__main__':
    main()